{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%classpath add jar ../resources/jar/flint-assembly-0.2.0-SNAPSHOT.jar\n",
    "\n",
    "%classpath add mvn org.apache.spark spark-sql_2.11 2.2.1\n",
    "%classpath add mvn org.apache.spark spark-mllib_2.11 2.2.1\n",
    "%classpath add mvn org.scalanlp breeze_2.10 0.13.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Creates spark session\n",
    "\n",
    "import com.twosigma.flint.timeseries.CSV\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "                        .appName(\"Simple Application\")\n",
    "                        .master(\"local[4]\")\n",
    "                        .config(\"spark.ui.enabled\", \"false\")\n",
    "                        .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "//Creates a TimeSeriesRDD from a gzipped CSV file\n",
    "\n",
    "val tsRdd = CSV.from(\n",
    "  spark.sqlContext,\n",
    "  \"../resources/data/zipped.csv.gz\",\n",
    "  header = true,\n",
    "  dateFormat = \"yyyyMMdd HH:mm:ss.SSS\",\n",
    "  codec = \"gzip\",\n",
    "  sorted = true\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Basic operations\n",
    "\n",
    "import org.apache.spark.sql.types.IntegerType\n",
    "import org.apache.spark.sql.types.DoubleType\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "def changeTimeFunction(id: Int, time: Long) : Long = {\n",
    "    return if (id == 3) time + 25 else time\n",
    "}\n",
    "\n",
    "val priceAsInteger = tsRdd.cast(\"price\" -> IntegerType)\n",
    "val filteredRowsByPrice = tsRdd.keepRows { row: Row => row.getAs[Double](\"price\") > 4.0 }\n",
    "val timeColumnOnly = tsRdd.keepColumns(\"time\")\n",
    "val withoutIdColumn = tsRdd.deleteColumns(\"id\")\n",
    "val renamedColumns = tsRdd.renameColumns(\"id\" -> \"ticker\", \"price\" -> \"highPrice\")\n",
    "val updatedTimeColumn = tsRdd.setTime {\n",
    "  row: Row =>\n",
    "    changeTimeFunction(row.getAs[Int](\"id\"), row.getAs[Long](\"time\"))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Crate columns\n",
    "val newHighPriceColumn = tsRdd.addColumns(\n",
    "  \"highPrice\" -> DoubleType -> {\n",
    "    r: Row => r.getAs[Double](\"price\") + 1.5\n",
    "  }\n",
    ")\n",
    "\n",
    "val results = tsRdd.addColumnsForCycle(\n",
    "  \"adjustedPrice\" -> DoubleType -> { rows: Seq[Row] =>\n",
    "    rows.map { row => (row, row.getAs[Double](\"price\") * rows.size) }.toMap\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "//Group functions\n",
    "import com.twosigma.flint.timeseries.Windows\n",
    "val groupedByCycle = tsRdd.groupByCycle()\n",
    "\n",
    "val intervals = tsRdd\n",
    ".keepRows { row: Row => row.getAs[Long](\"time\") % 100 == 0 }\n",
    ".keepRows { row: Row => row.getAs[Int](\"id\") == 3}\n",
    ".keepColumns(\"time\")\n",
    "\n",
    "val groupedByInterval = tsRdd.groupByInterval(intervals)\n",
    "val groupedByWindows = tsRdd.addWindows(Windows.pastAbsoluteTime(\"1000ns\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Temporal join functions\n",
    "val leftTSRdd = tsRdd.keepRows { row: Row => row.getAs[Long](\"time\") % 100 == 0 }\n",
    ".keepColumns(\"time\", \"price\")\n",
    "val rightTSRdd = tsRdd.keepRows { row: Row => row.getAs[Long](\"time\") % 100 != 0 }\n",
    ".keepColumns(\"time\", \"id\")\n",
    "\n",
    "val leftJoin = leftTSRdd.leftJoin(rightTSRdd, tolerance = \"50ns\")\n",
    "val futureLeftJoin = leftTSRdd.futureLeftJoin(rightTSRdd, tolerance = \"50ns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Summarize functions\n",
    "import com.twosigma.flint.timeseries.Summarizers\n",
    "val summarizedCycles = tsRdd.summarizeCycles(Summarizers.sum(\"price\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//stat.regression\n",
    "\n",
    "import breeze.linalg.DenseVector\n",
    "import org.apache.spark.mllib.random.RandomRDDs\n",
    "import com.twosigma.flint.math.stats.regression.WeightedLabeledPoint\n",
    "import com.twosigma.flint.math.stats.regression.OLSMultipleLinearRegression\n",
    "\n",
    "// Generate a random data set from a linear model with beta = [1.0, 2.0] and intercept = 3.0\n",
    "val data = WeightedLabeledPoint.generateSampleData(spark.sparkContext, DenseVector(1.0, 2.0), 3.0)\n",
    "\n",
    "// Fit the data using the OLS linear regression.\n",
    "val model = OLSMultipleLinearRegression.regression(data)\n",
    "\n",
    "// Retrieve the estimate beta and intercept.\n",
    "val denseVector = model.estimateRegressionParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "",
   "name": "Scala",
   "nbconverter_exporter": "",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
